sources:
  http_logs:
    type: http_server
    address: "0.0.0.0:${VECTOR_PORT:-8080}"
    method: POST
    decoding:
      codec: json
    framing:
      method: bytes

transforms:
  # Prepare events for multi-line merging
  # Extract log content and create a grouping key to identify related log fragments
  prepare_multiline:
    type: remap
    inputs: [http_logs]
    source: |
      # Extract the log message (could be in 'log' field or 'message' field)
      log_content = if exists(.log) { string!(.log) } else if exists(.message) { string!(.message) } else { string!(.) }
      
      # Check if the log content appears to be incomplete JSON (doesn't end with })
      is_complete = match(log_content, r'^.*\}$')
      .is_complete = is_complete == true
      
      # Store the log content and completion status
      .log_content = log_content
      
      # Create a grouping key based on timestamp and service/app to group related log fragments
      # This ensures we only merge logs from the same source and timeframe
      timestamp_str = if exists(.@timestamp) { string!(.@timestamp) } else if exists(.timestamp) { string!(.timestamp) } else { "" }
      service_str = if exists(.service) { string!(.service) } else if exists(.app) { string!(.app) } else { "" }
      .group_key = timestamp_str + "|" + service_str

  # Merge consecutive incomplete JSON entries using reduce transform with merge_strategies
  merge_multiline:
    type: reduce
    inputs: [prepare_multiline]
    group_by:
      - group_key
    merge_strategies:
      log_content: concat
    expire_after_ms: 5000
    max_events: 100
    ends_when: |
      match(string!(.log_content), r'^.*\}$') == true

  # Process merged events and parse JSON
  parse_json:
    type: remap
    inputs: [merge_multiline]
    source: |
      # Get merged log content (reduce will concatenate log_content fields)
      log_content = if exists(.log_content) { string!(.log_content) } else if exists(.log) { string!(.log) } else if exists(.message) { string!(.message) } else { "" }
      
      # Check if merged content is complete JSON
      is_complete_json = match(log_content, r'^.*\}$')
      is_complete = is_complete_json == true
      
      # Try to parse the log content as JSON if it appears complete
      if is_complete {
        parsed, parse_err = parse_json(log_content)
        if parse_err == null && is_object(parsed) {
          # Store parsed JSON as a nested object for Datadog
          .parsed_json = parsed
        }
        # Keep the original log field for Datadog
        .log = log_content
      } else {
        # If still incomplete, just pass through the log content
        .log = log_content
      }
      
      # Remove helper fields
      del(.log_content)
      del(.is_complete)
      del(.group_key)

sinks:
  datadog:
    type: datadog_logs
    inputs: [parse_json]
    default_api_key: "${DD_API_KEY}"
    site: "${DD_SITE:-datadoghq.com}"
    compression: gzip
    batch:
      max_events: 1000
      timeout_secs: 5
