sources:
  http_logs:
    type: http_server
    address: "0.0.0.0:${VECTOR_PORT:-8080}"
    method: POST
    decoding:
      codec: json
    framing:
      method: bytes

transforms:
  # Prepare events for multi-line merging
  # Extract log content and create a grouping key to identify related log fragments
  prepare_multiline:
    type: remap
    inputs: [http_logs]
    source: |
      # Extract the log message (could be in 'log' field or 'message' field)
      log_content = .log ?? .message ?? string!(.)
      
      # Check if the log content appears to be incomplete JSON (doesn't end with })
      is_complete = match(log_content, r'^.*\}$')
      
      # Store the log content and completion status
      .log_content = log_content
      .is_complete = is_complete ?? false
      
      # Create a grouping key based on timestamp and service/app to group related log fragments
      # This ensures we only merge logs from the same source and timeframe
      timestamp_str = string!(.@timestamp ?? .timestamp ?? "")
      service_str = string!(.service ?? .app ?? "")
      .group_key = timestamp_str + "|" + service_str

  # Merge consecutive incomplete JSON entries using reduce transform
  merge_multiline:
    type: reduce
    inputs: [prepare_multiline]
    group_by:
      - group_key
    reduce:
      source: |
        # Concatenate all log_content fields from events in the group
        merged_log = ""
        for event in .events {
          merged_log = merged_log + event.log_content
        }
        
        # Check if merged log is now complete JSON
        is_complete_json = match(merged_log, r'^.*\}$')
        
        # Store merged content
        .log_content = merged_log
        .is_complete = is_complete_json ?? false
        
        # Copy fields from first event (preserving metadata)
        first_event = .events[0]
        for key, value in first_event {
          if key != "log_content" && key != "is_complete" && key != "group_key" {
            .[key] = value
          }
        }
        
        # Update log/message field with merged content
        if exists(.log) {
          .log = merged_log
        } else if exists(.message) {
          .message = merged_log
        } else {
          .message = merged_log
        }

  # Parse the merged JSON log content and clean up helper fields
  parse_json:
    type: remap
    inputs: [merge_multiline]
    source: |
      log_content = .log ?? .message ?? ""
      
      # Try to parse the log content as JSON if it appears complete
      if .is_complete {
        parsed = parse_json(log_content) ?? {}
        
        # Merge parsed JSON fields into the event
        for key, value in parsed {
          .[key] = value
        }
        
        # Keep the original log field for Datadog
        .log = log_content
      }
      
      # Remove helper fields
      del(.log_content)
      del(.is_complete)
      del(.group_key)

sinks:
  datadog:
    type: datadog_logs
    inputs: [parse_json]
    default_api_key: "${DD_API_KEY}"
    site: "${DD_SITE:-datadoghq.com}"
    compression: gzip
    batch:
      max_events: 1000
      timeout_secs: 5
