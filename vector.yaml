sources:
  http_logs:
    type: http_server
    address: "0.0.0.0:${VECTOR_PORT:-8080}"
    # Note: Not restricting method to allow GET requests for health checks
    # GET requests (health checks) won't have bodies, so they won't create log events
    # but will return 200 OK. POST requests with JSON bodies will be processed as logs.
    decoding:
      codec: json
    framing:
      method: bytes

transforms:
  # Classify logs as AUDIT (multi-line) or regular (single-line)
  classify_logs:
    type: remap
    inputs: [http_logs]
    source: |
      # Extract the log content
      log_content = if exists(.log) { string!(.log) } else if exists(.message) { string!(.message) } else { "" }
      .log_content = log_content
      
      # Detect if this is an AUDIT log start line
      # Pattern: "YYYY-MM-DD HH:MM:SS UTC LOG:  AUDIT:"
      is_audit_start = match(log_content, r'LOG:\s+AUDIT:')
      .is_audit_start = is_audit_start
      
      # Detect if this is a continuation line (starts with tab character \t)
      # The \u0009 in JSON becomes a tab character
      is_continuation = match(log_content, r'^\t') || match(log_content, r'^\s{2,}')
      .is_continuation = is_continuation
      
      # Detect if this is the end of an AUDIT entry (contains "<not logged>")
      is_audit_end = match(log_content, r'<not logged>')
      .is_audit_end = is_audit_end
      
      # Mark as AUDIT-related if it's a start, continuation, or end
      .is_audit_related = is_audit_start || is_continuation || is_audit_end
      
      # Preserve the original time field for grouping
      .original_time = if exists(.time) { .time } else { "" }

  # Route AUDIT logs vs regular logs
  route_logs:
    type: route
    inputs: [classify_logs]
    route:
      audit: .is_audit_related == true
      regular: .is_audit_related == false

  # Merge multi-line AUDIT logs
  # Uses starts_when to detect new AUDIT entries and merge until the next one
  merge_audit_logs:
    type: reduce
    inputs: [route_logs.audit]
    group_by: []
    merge_strategies:
      log_content: concat_newline
      original_time: retain
      stream: retain
      time: retain
    starts_when: |
      # A new AUDIT entry starts when we see "LOG:  AUDIT:" pattern
      match(string!(.log_content), r'LOG:\s+AUDIT:')
    expire_after_ms: 10000
    flush_period_ms: 5000

  # Process merged AUDIT logs
  process_audit:
    type: remap
    inputs: [merge_audit_logs]
    source: |
      # Get the merged log content
      log_content = if exists(.log_content) { string!(.log_content) } else { "" }
      
      # Clean up the log content - replace literal \n with actual newlines if needed
      # and normalize whitespace
      .message = log_content
      
      # Extract AUDIT metadata using regex
      # Pattern: "YYYY-MM-DD HH:MM:SS UTC LOG:  AUDIT: SESSION,N,N,TYPE,OPERATION,,,\"SQL\""
      audit_match, err = parse_regex(log_content, r'(?P<timestamp>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2} UTC) LOG:\s+AUDIT:\s+(?P<audit_class>\w+),(?P<statement_id>\d+),(?P<substatement_id>\d+),(?P<class>\w+),(?P<command>\w+),,,')
      
      if err == null {
        .audit = {
          "timestamp": audit_match.timestamp,
          "audit_class": audit_match.audit_class,
          "statement_id": audit_match.statement_id,
          "substatement_id": audit_match.substatement_id,
          "class": audit_match.class,
          "command": audit_match.command
        }
        .log_type = "pg_audit"
      } else {
        .log_type = "pg_audit_unknown"
      }
      
      # Set the log field for Datadog
      .log = log_content
      
      # Clean up helper fields
      del(.log_content)
      del(.is_audit_start)
      del(.is_continuation)
      del(.is_audit_end)
      del(.is_audit_related)

  # Process regular (non-AUDIT) logs - pass through with minimal processing
  process_regular:
    type: remap
    inputs: [route_logs.regular]
    source: |
      # Get the log content
      log_content = if exists(.log_content) { string!(.log_content) } else { "" }
      
      # Set the message and log fields
      .message = log_content
      .log = log_content
      .log_type = "postgres"
      
      # Clean up helper fields
      del(.log_content)
      del(.is_audit_start)
      del(.is_continuation)
      del(.is_audit_end)
      del(.is_audit_related)

sinks:
  # Uncomment below for local debugging (shows processed logs in container output)
  # console_debug:
  #   type: console
  #   inputs: [process_audit, process_regular]
  #   encoding:
  #     codec: json

  datadog:
    type: datadog_logs
    inputs: [process_audit, process_regular]
    default_api_key: "${DD_API_KEY}"
    site: "${DD_SITE:-datadoghq.com}"
    compression: gzip
    batch:
      max_events: 1000
      timeout_secs: 5
